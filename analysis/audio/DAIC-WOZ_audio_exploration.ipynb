{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librosa\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# participant_id = 402\n",
    "# dir = \"../../data/DAIC-WOZ/\"\n",
    "# audio_path = dir + f\"{participant_id}_P/{participant_id}_AUDIO.wav\"\n",
    "# transcript_path = dir + f\"{participant_id}_P/{participant_id}_TRANSCRIPT.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the audio file\n",
    "# y, sr = librosa.load(audio_path, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_audio(data, sr):\n",
    "#     time = np.arange(len(data)) / sr\n",
    "#     plt.plot(time, data)\n",
    "#     plt.xlabel('Time (seconds)')\n",
    "#     plt.ylabel('Amplitude')\n",
    "#     plt.show()\n",
    "\n",
    "# plot_audio(y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio(data=y, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcript_df = pd.read_csv(\n",
    "#                     transcript_path,\n",
    "#                     sep=r\"\\s+\",  # One or more whitespaces as the delimiter\n",
    "#                     engine=\"python\",\n",
    "#                     skiprows=1,  # Skip the first header row\n",
    "#                     header=None,\n",
    "#                     usecols=[0, 1, 2],  # Only load the columns we care about\n",
    "#                     names=['start_time', 'stop_time', 'speaker'],\n",
    "#                 )\n",
    "\n",
    "# if participant_id == 373:\n",
    "#     cond = (transcript_df['start_time'] >= 5*60 + 52) & (transcript_df['stop_time'] <= 7*60)\n",
    "#     transcript_df = transcript_df.drop(transcript_df[cond].index)\n",
    "#     print(\"373_P interruption removed\")\n",
    "\n",
    "# elif participant_id == 444:\n",
    "#     cond = (transcript_df['start_time'] >= 4*60 + 46) & (transcript_df['stop_time'] <= 6*60 + 27)\n",
    "#     transcript_df = transcript_df.drop(transcript_df[cond].index)\n",
    "#     print(\"444_P interruption removed\")\n",
    "\n",
    "# elif participant_id == 402:\n",
    "#     cond = (transcript_df[\"start_time\"] > 833.267)\n",
    "#     transcript_df = transcript_df.drop(transcript_df[cond].index)\n",
    "\n",
    "# elif participant_id == 420:\n",
    "#     cond = (transcript_df[\"start_time\"] > 706.367)\n",
    "#     transcript_df = transcript_df.drop(transcript_df[cond].index)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcript_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracted Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_speaker = []\n",
    "# indices = []\n",
    "\n",
    "# for index, row in transcript_df.iterrows():\n",
    "#     start_time = row['start_time']\n",
    "#     stop_time = row['stop_time'] \n",
    "#     speaker = row[\"speaker\"]\n",
    "\n",
    "#     if speaker == \"Ellie\":\n",
    "#         continue\n",
    "\n",
    "#     print(index, end=\"\\r\")\n",
    "\n",
    "#     # Extract the audio segment\n",
    "#     start_sample = int(start_time * sr)\n",
    "#     stop_sample = int(stop_time * sr)\n",
    "#     segment = y[start_sample:stop_sample]\n",
    "\n",
    "#     y_speaker.append(segment)\n",
    "#     indices.append(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking individual segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy_segment = 34 # 34 for 300_P\n",
    "# plot_audio(y_speaker[noisy_segment], sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio(data=y_speaker[noisy_segment], rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenated Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.concatenate(y_speaker))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio(np.concatenate(y_speaker), rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchaudio\n",
    "# import librosa\n",
    "# from silero_vad import get_speech_timestamps, read_audio\n",
    "\n",
    "# def non_voiced_removal(audio, sample_rate):\n",
    "#     \"\"\"\n",
    "#     Removes non-voiced parts using Silero VAD from a librosa array.\n",
    "\n",
    "#     Args:\n",
    "#         audio (np.ndarray): Audio signal loaded with librosa (1D array).\n",
    "#         sample_rate (int): Sampling rate of the audio (must be 16kHz).\n",
    "\n",
    "#     Returns:\n",
    "#         np.ndarray: Cleaned audio signal (speech segments concatenated).\n",
    "#     \"\"\"\n",
    "#     # Convert to PyTorch tensor\n",
    "#     audio_tensor = torch.tensor(audio, dtype=torch.float32)\n",
    "\n",
    "#     # Load Silero VAD model\n",
    "#     model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', source='github')\n",
    "#     get_speech_timestamps = utils[0]\n",
    "\n",
    "#     # Detect speech segments\n",
    "#     speech_timestamps = get_speech_timestamps(audio_tensor, model, sampling_rate=sample_rate)\n",
    "\n",
    "#     # Extract speech segments\n",
    "#     speech_audio = torch.cat([audio_tensor[segment['start']:segment['end']] for segment in speech_timestamps])\n",
    "\n",
    "#     return speech_audio.numpy()\n",
    "\n",
    "# # Remove noise using Silero\n",
    "# cleaned_audio = non_voiced_removal(np.concatenate(y_speaker), sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_audio(cleaned_audio, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio(cleaned_audio, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_mfcc = 60\n",
    "# window_length = int(0.06 * sr)\n",
    "# hop_length = int(0.06 * sr)\n",
    "# n_mels = 60\n",
    "\n",
    "\n",
    "# # Compute the MFCCs\n",
    "# mfccs = librosa.feature.mfcc(y=cleaned_audio, sr=sr, n_mfcc=n_mfcc, n_fft=window_length, hop_length=hop_length, window=\"hamming\", n_mels=n_mels)\n",
    "# mfccs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_mfcc(mfcc_data):\n",
    "#     \"\"\"\n",
    "#     Loads an MFCC file and plots the MFCC features.\n",
    "#     Args:\n",
    "#         mfcc_file (str): Path to the MFCC file (npy format).\n",
    "#     \"\"\" \n",
    "#     # Plot the MFCC features\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     plt.imshow(mfcc_data, aspect='auto', origin='lower', cmap='viridis')\n",
    "#     plt.title('MFCC')\n",
    "#     plt.xlabel('Frames')\n",
    "#     plt.ylabel('MFCC Coefficients')\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "\n",
    "# Example usage\n",
    "# plot_mfcc(mfccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
