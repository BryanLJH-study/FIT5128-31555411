{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from modules.utils import load_data, load_config, test_model\n",
    "from modules.trainer_tester import TrainerTester\n",
    "\n",
    "from modules.monash_data_pipeline import GeneralizationTestingDataPipeline \n",
    "\n",
    "from models.au_mfcc.model import DepressionDetectionModel as au_mfcc_model\n",
    "from models.au_only.model import DepressionDetectionModel as au_model\n",
    "from models.mfcc_only.model import DepressionDetectionModel as mfcc_model\n",
    "\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading AU-MFCC Paired Data\n",
    "\n",
    "The dataloaders here will be used for the various modalities moving forward for consistency.\n",
    "\n",
    "This means that there will be some discrepencies for the individual (AU only and MFCC only) modalities. Such as having a few less samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load AU-MFCC config (include gender)\n",
    "config = load_config(\"./models/au_mfcc/logs/config.json\")\n",
    "config[\"DataPipeline\"][\"include_gender\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DAIC-WOZ Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DAIC-WOZ Data\n",
      "Unsuccessful frames in data to be removed: 3.835461318536759%\n",
      "\n",
      "Unique videos: 189\n",
      "Total processed frames: 2362555\n",
      "Avg frames per video: 12500.291005291005\n",
      "Memory used: 0.5368733964860439 GB\n",
      "\n",
      "Preparing Dataloader\n",
      "Preparing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 107/107 [00:28<00:00,  3.70it/s]\n",
      "Val: 100%|██████████| 35/35 [00:09<00:00,  3.60it/s]\n",
      "Test: 100%|██████████| 47/47 [00:13<00:00,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DAIC-WOZ Data Distribution\n",
      "0:240, 1:93\n",
      "ratio: 2.5806451612903225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load DAIC-WOZ Data\n",
    "dw_dataloaders, split_dfs = load_data(config)\n",
    "\n",
    "cat_0 = 0\n",
    "cat_1 = 0\n",
    "\n",
    "for sample_no in range(len(dw_dataloaders[\"test\"].dataset)):\n",
    "    if dw_dataloaders[\"test\"].dataset[sample_no][\"Category\"] == 0:\n",
    "        cat_0 += 1\n",
    "    else:\n",
    "        cat_1 += 1\n",
    "\n",
    "print(\"\\nDAIC-WOZ Data Distribution\")\n",
    "print(f\"0:{cat_0}, 1:{cat_1}\")\n",
    "print(f\"ratio: {cat_0/cat_1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Behavioural Dataset (For Further Generalization Testing)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TEST: 100%|██████████| 7/7 [00:01<00:00,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Behavioural Dataset Data Distribution\n",
      "0:15, 1:32\n",
      "ratio: 0.46875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Behavioural dataset\n",
    "daic_woz_train_split_df = pd.read_csv(\"../data/au_mfcc/DAIC-WOZ_Participant_Voiced/train_split_Depression_AVEC2017.csv\")\n",
    "\n",
    "gen_data_pipeline = GeneralizationTestingDataPipeline(\n",
    "    au_dir=\"../data/au_mfcc/OpenFaceAnnotations_Participant_Voiced/\",\n",
    "    mfcc_dir=\"../data/au_mfcc/MFCCAnnotations/60_60_60_60/\", \n",
    "    keep_AU_cols=config[\"DAIC_WOZ_READER\"][\"keep_AU_cols\"], \n",
    "    daic_woz_train_split_df=daic_woz_train_split_df,\n",
    "    au_separate=config[\"DataPipeline\"][\"au_separate\"],\n",
    "    au_fixed_length=config[\"DataPipeline\"][\"au_fixed_length\"],\n",
    "    mfcc_fixed_length=config[\"DataPipeline\"][\"mfcc_fixed_length\"],\n",
    "    daic_woz_mfcc_dir=config[\"DataPipeline\"][\"mfcc_dir\"],\n",
    "    segment_config=config[\"DataPipeline\"][\"segment_config\"],\n",
    "    batch_size=config[\"DataPipeline\"][\"batch_size\"]\n",
    ")\n",
    "    \n",
    "gen_dataloaders = gen_data_pipeline.dataloaders\n",
    "\n",
    "cat_0 = 0\n",
    "cat_1 = 0\n",
    "\n",
    "for sample_no in range(len(gen_dataloaders[\"test\"].dataset)):\n",
    "    if gen_dataloaders[\"test\"].dataset[sample_no][\"Category\"] == 0:\n",
    "        cat_0 += 1\n",
    "    else:\n",
    "        cat_1 += 1\n",
    "\n",
    "print(\"\\nBehavioural Dataset Data Distribution\")\n",
    "print(f\"0:{cat_0}, 1:{cat_1}\")\n",
    "print(f\"ratio: {cat_0/cat_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, version):\n",
    "    # Input Adapter\n",
    "    def input_adapter(batch: dict) -> dict:\n",
    "        inputs = {}\n",
    "        if \"au\" in version.split(\"_\"):\n",
    "            inputs['au_input'] = batch['AUs'].to(device)\n",
    "        if \"mfcc\" in version.split(\"_\"):\n",
    "            inputs['mfcc_input'] = batch['MFCCs'].to(device)  \n",
    "        return inputs\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # DAIC\n",
    "    print(\"DAIC-WOZ TEST SPLIT DATA\")\n",
    "    # Load model\n",
    "    trainer = TrainerTester(model=model, dataloaders=dw_dataloaders, device=device, criterion=criterion, optimizer=None, input_adapter=input_adapter)\n",
    "    # Test model\n",
    "    test_metrics = trainer.test(f\"./models/{version}/checkpoints/final_model.pth\")\n",
    "    print(\"\\nConfusion Matrix\\n\", test_metrics[\"confusion_matrix\"])\n",
    "    print(\"\\nClassification Report\\n\", test_metrics[\"classification_report\"], \"\\n\")\n",
    "\n",
    "\n",
    "    # Monash\n",
    "    print(\"BEHAVOIURAL DATASET DATA\")\n",
    "    # Load model\n",
    "    trainer = TrainerTester(model=model, dataloaders=gen_dataloaders, device=device, criterion=criterion, optimizer=None, input_adapter=input_adapter)\n",
    "    # Test model\n",
    "    test_metrics = trainer.test(f\"./models/{version}/checkpoints/final_model.pth\")\n",
    "    print(\"\\nConfusion Matrix\\n\", test_metrics[\"confusion_matrix\"])\n",
    "    print(\"\\nClassification Report\\n\", test_metrics[\"classification_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AU_MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAIC-WOZ TEST SPLIT DATA\n",
      "Loaded model states from ./models/au_mfcc/checkpoints/final_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bryan\\Desktop\\Projects\\Multi-Modal-Depression-Detection\\depression_detection\\modules\\trainer_tester.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix\n",
      " [[228  12]\n",
      " [ 77  16]]\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.95      0.84       240\n",
      "           1       0.57      0.17      0.26        93\n",
      "\n",
      "    accuracy                           0.73       333\n",
      "   macro avg       0.66      0.56      0.55       333\n",
      "weighted avg       0.70      0.73      0.68       333\n",
      " \n",
      "\n",
      "BEHAVOIURAL DATASET DATA\n",
      "Loaded model states from ./models/au_mfcc/checkpoints/final_model.pth\n",
      "\n",
      "Confusion Matrix\n",
      " [[ 5 10]\n",
      " [10 22]]\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.33      0.33        15\n",
      "           1       0.69      0.69      0.69        32\n",
      "\n",
      "    accuracy                           0.57        47\n",
      "   macro avg       0.51      0.51      0.51        47\n",
      "weighted avg       0.57      0.57      0.57        47\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bryan\\Desktop\\Projects\\Multi-Modal-Depression-Detection\\depression_detection\\modules\\trainer_tester.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=self.device)\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "config = load_config(\"./models/au_mfcc/logs/config.json\")\n",
    "au_mfcc_model_trained = au_mfcc_model(**config[\"Model\"])\n",
    "\n",
    "test(au_mfcc_model_trained, \"au_mfcc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AU Only\n",
    "\n",
    "No missing samples when using AU features from AU-MFCC paired version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAIC-WOZ TEST SPLIT DATA\n",
      "Loaded model states from ./models/au_only/checkpoints/final_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bryan\\Desktop\\Projects\\Multi-Modal-Depression-Detection\\depression_detection\\modules\\trainer_tester.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix\n",
      " [[182  58]\n",
      " [ 73  20]]\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.76      0.74       240\n",
      "           1       0.26      0.22      0.23        93\n",
      "\n",
      "    accuracy                           0.61       333\n",
      "   macro avg       0.49      0.49      0.48       333\n",
      "weighted avg       0.59      0.61      0.60       333\n",
      " \n",
      "\n",
      "BEHAVOIURAL DATASET DATA\n",
      "Loaded model states from ./models/au_only/checkpoints/final_model.pth\n",
      "\n",
      "Confusion Matrix\n",
      " [[13  2]\n",
      " [27  5]]\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.87      0.47        15\n",
      "           1       0.71      0.16      0.26        32\n",
      "\n",
      "    accuracy                           0.38        47\n",
      "   macro avg       0.52      0.51      0.36        47\n",
      "weighted avg       0.59      0.38      0.33        47\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bryan\\Desktop\\Projects\\Multi-Modal-Depression-Detection\\depression_detection\\modules\\trainer_tester.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=self.device)\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "config = load_config(\"./models/au_only/logs/config.json\")\n",
    "au_model_trained = au_model(**config[\"Model\"])\n",
    "\n",
    "test(au_model_trained, \"au_only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCC Only\n",
    "\n",
    "Note: Has one less depressed sample because of dataloader pairing in AU-MFCC vs MFCC only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAIC-WOZ TEST SPLIT DATA\n",
      "Loaded model states from ./models/mfcc_only/checkpoints/final_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bryan\\Desktop\\Projects\\Multi-Modal-Depression-Detection\\depression_detection\\modules\\trainer_tester.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix\n",
      " [[194  46]\n",
      " [ 70  23]]\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.81      0.77       240\n",
      "           1       0.33      0.25      0.28        93\n",
      "\n",
      "    accuracy                           0.65       333\n",
      "   macro avg       0.53      0.53      0.53       333\n",
      "weighted avg       0.62      0.65      0.63       333\n",
      " \n",
      "\n",
      "BEHAVOIURAL DATASET DATA\n",
      "Loaded model states from ./models/mfcc_only/checkpoints/final_model.pth\n",
      "\n",
      "Confusion Matrix\n",
      " [[ 4 11]\n",
      " [ 8 24]]\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.27      0.30        15\n",
      "           1       0.69      0.75      0.72        32\n",
      "\n",
      "    accuracy                           0.60        47\n",
      "   macro avg       0.51      0.51      0.51        47\n",
      "weighted avg       0.57      0.60      0.58        47\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bryan\\Desktop\\Projects\\Multi-Modal-Depression-Detection\\depression_detection\\modules\\trainer_tester.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=self.device)\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "config = load_config(\"./models/mfcc_only/logs/config.json\")\n",
    "mfcc_model_trained = mfcc_model(**config[\"Model\"])\n",
    "\n",
    "test(mfcc_model_trained, \"mfcc_only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predictions(model, version, dataloaders):\n",
    "    # Input Adapter\n",
    "    def input_adapter(batch: dict) -> dict:\n",
    "        inputs = {}\n",
    "        if \"au\" in version.split(\"_\"):\n",
    "            inputs['au_input'] = batch['AUs'].to(device)\n",
    "        if \"mfcc\" in version.split(\"_\"):\n",
    "            inputs['mfcc_input'] = batch['MFCCs'].to(device)  \n",
    "        return inputs\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Load model\n",
    "    trainer = TrainerTester(model=model, dataloaders=dataloaders, device=device, criterion=criterion, optimizer=None, input_adapter=input_adapter)\n",
    "    # Get predictions\n",
    "    predictions = trainer.test_predictions()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def mcnemear_test(true_labels, model_1_preds, model_2_preds):\n",
    "    contingency_table = mcnemar_table(y_target=true_labels, \n",
    "                                    y_model1=model_1_preds, \n",
    "                                    y_model2=model_2_preds)\n",
    "\n",
    "    result = mcnemar(contingency_table, exact=True)\n",
    "\n",
    "    return result.pvalue\n",
    "\n",
    "\n",
    "def performance_comparison(trained_models, versions, dataloaders):\n",
    "    # Get true labels and predictions of each model\n",
    "    true_labels = None\n",
    "    predictions = []\n",
    "    for i in range(len(versions)):\n",
    "        preds, labels = model_predictions(trained_models[i], versions[i], dataloaders)\n",
    "\n",
    "        if true_labels is None:\n",
    "            true_labels = np.array(labels)\n",
    "\n",
    "        predictions.append(np.array(preds))\n",
    "\n",
    "    # Perform McNemar test on  pair-wise combinations of models\n",
    "    for i in range(len(versions)):\n",
    "        for j in range(i+1, len(versions)):\n",
    "            p_value = mcnemear_test(true_labels, predictions[i], predictions[j])\n",
    "            print(f\"p-value of {versions[i]} & {versions[j]}: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAIC-WOZ Dataset McNemar Test\n",
      "p-value of au_only & mfcc_only: 0.18759872474577577\n",
      "p-value of au_only & au_mfcc: 1.0902049191089246e-05\n",
      "p-value of mfcc_only & au_mfcc: 0.0001980394513374506\n"
     ]
    }
   ],
   "source": [
    "versions = [\"au_only\", \"mfcc_only\", \"au_mfcc\"]\n",
    "trained_models = [au_model_trained, mfcc_model_trained, au_mfcc_model_trained]\n",
    "\n",
    "print(\"DAIC-WOZ Dataset McNemar Test\")\n",
    "performance_comparison(trained_models, versions, dw_dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavioural Dataset McNemar Test\n",
      "p-value of au_only & mfcc_only: 0.09873714670538905\n",
      "p-value of au_only & au_mfcc: 0.12207812070846558\n",
      "p-value of mfcc_only & au_mfcc: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Behavioural Dataset McNemar Test\")\n",
    "performance_comparison(trained_models, versions, gen_dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = []\n",
    "\n",
    "for sample_no in range(len(dw_dataloaders[\"test\"].dataset)):\n",
    "    genders.append(dw_dataloaders[\"test\"].dataset[sample_no][\"Gender\"])\n",
    "\n",
    "genders = np.array(genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, labels = model_predictions(trained_models[2], versions[2], dw_dataloaders)\n",
    "preds = np.array(preds)\n",
    "labels = np.array(labels)\n",
    "\n",
    "male_idx = np.where(genders == 0)[0]\n",
    "male_preds = preds[male_idx]\n",
    "male_labels = labels[male_idx]\n",
    "\n",
    "female_idx = np.where(genders == 1)[0]\n",
    "female_preds = preds[female_idx]\n",
    "female_labels = labels[female_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix\n",
      " [[132   7]\n",
      " [ 40  16]]\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.95      0.85       139\n",
      "           1       0.70      0.29      0.41        56\n",
      "\n",
      "    accuracy                           0.76       195\n",
      "   macro avg       0.73      0.62      0.63       195\n",
      "weighted avg       0.75      0.76      0.72       195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Males\n",
    "cm = confusion_matrix(male_labels, male_preds)\n",
    "report = classification_report(male_labels, male_preds)\n",
    "\n",
    "print(\"\\nConfusion Matrix\\n\", cm)\n",
    "print(\"\\nClassification Report\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix\n",
      " [[96  5]\n",
      " [37  0]]\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.95      0.82       101\n",
      "           1       0.00      0.00      0.00        37\n",
      "\n",
      "    accuracy                           0.70       138\n",
      "   macro avg       0.36      0.48      0.41       138\n",
      "weighted avg       0.53      0.70      0.60       138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Females\n",
    "cm = confusion_matrix(female_labels, female_preds)\n",
    "report = classification_report(female_labels, female_preds)\n",
    "\n",
    "print(\"\\nConfusion Matrix\\n\", cm)\n",
    "print(\"\\nClassification Report\\n\", report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
