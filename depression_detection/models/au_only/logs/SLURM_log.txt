Running Script: /ibm/gpfs/home/blea0003/Multi-Modal-Depression-Detection/depression_detection/models/au_only
Reading DAIC-WOZ Data
Reading Participant ID: 367...Reading Participant ID: 485...Reading Participant ID: 448...Reading Participant ID: 451...Reading Participant ID: 414...Reading Participant ID: 362...Reading Participant ID: 325...Reading Participant ID: 320...Reading Participant ID: 489...Reading Participant ID: 492...Reading Participant ID: 455...Reading Participant ID: 418...Reading Participant ID: 366...Reading Participant ID: 329...Reading Participant ID: 450...Reading Participant ID: 413...Reading Participant ID: 361...Reading Participant ID: 459...Reading Participant ID: 336...Reading Participant ID: 491...Reading Participant ID: 454...Reading Participant ID: 417...Reading Participant ID: 331...Reading Participant ID: 377...Reading Participant ID: 458...Reading Participant ID: 424...Reading Participant ID: 372...Reading Participant ID: 335...Reading Participant ID: 490...Reading Participant ID: 330...Reading Participant ID: 465...Reading Participant ID: 428...Reading Participant ID: 376...Reading Participant ID: 339...Reading Participant ID: 423...Reading Participant ID: 371...Reading Participant ID: 334...Reading Participant ID: 300...Reading Participant ID: 469...Reading Participant ID: 309...Reading Participant ID: 464...Reading Participant ID: 427...Reading Participant ID: 375...Reading Participant ID: 341...Reading Participant ID: 304...Reading Participant ID: 422...Reading Participant ID: 468...Reading Participant ID: 382...Reading Participant ID: 345...Reading Participant ID: 308...Reading Participant ID: 463...Reading Participant ID: 340...Reading Participant ID: 303...Reading Participant ID: 438...Reading Participant ID: 386...Reading Participant ID: 349...Reading Participant ID: 470...Reading Participant ID: 433...Reading Participant ID: 381...Reading Participant ID: 344...Reading Participant ID: 307...Reading Participant ID: 310...Reading Participant ID: 479...Reading Participant ID: 474...Reading Participant ID: 437...Reading Participant ID: 385...Reading Participant ID: 348...Reading Participant ID: 351...Reading Participant ID: 314...Reading Participant ID: 432...Reading Participant ID: 478...Reading Participant ID: 389...Reading Participant ID: 392...Reading Participant ID: 355...Reading Participant ID: 318...Reading Participant ID: 473...Reading Participant ID: 436...Reading Participant ID: 402...Reading Participant ID: 350...Reading Participant ID: 313...Reading Participant ID: 396...Reading Participant ID: 359...Reading Participant ID: 477...Reading Participant ID: 480...Reading Participant ID: 443...Reading Participant ID: 406...Reading Participant ID: 391...Reading Participant ID: 354...Reading Participant ID: 317...Reading Participant ID: 401...Reading Participant ID: 484...Reading Participant ID: 447...Reading Participant ID: 395...Reading Participant ID: 358...Reading Participant ID: 324...Reading Participant ID: 442...Reading Participant ID: 405...Reading Participant ID: 488...Reading Participant ID: 399...Reading Participant ID: 365...Reading Participant ID: 328...Reading Participant ID: 483...Reading Participant ID: 446...Reading Participant ID: 409...Reading Participant ID: 412...Reading Participant ID: 360...Reading Participant ID: 323...Reading Participant ID: 369...Reading Participant ID: 487...Reading Participant ID: 453...Reading Participant ID: 416...Reading Participant ID: 364...Reading Participant ID: 327...Reading Participant ID: 411...Reading Participant ID: 457...Reading Participant ID: 368...Reading Participant ID: 452...Reading Participant ID: 415...Reading Participant ID: 338...Reading Participant ID: 456...Reading Participant ID: 419...Reading Participant ID: 370...Reading Participant ID: 333...Reading Participant ID: 379...Reading Participant ID: 426...Reading Participant ID: 374...Reading Participant ID: 337...Reading Participant ID: 421...Reading Participant ID: 332...Reading Participant ID: 467...Reading Participant ID: 378...Reading Participant ID: 462...Reading Participant ID: 425...Reading Participant ID: 373...Reading Participant ID: 302...Reading Participant ID: 420...Reading Participant ID: 466...Reading Participant ID: 429...Reading Participant ID: 380...Reading Participant ID: 343...Reading Participant ID: 306...Reading Participant ID: 461...Reading Participant ID: 301...Reading Participant ID: 384...Reading Participant ID: 347...Reading Participant ID: 431...Reading Participant ID: 305...Reading Participant ID: 388...Reading Participant ID: 472...Reading Participant ID: 435...Reading Participant ID: 383...Reading Participant ID: 346...Reading Participant ID: 312...Reading Participant ID: 430...Reading Participant ID: 476...Reading Participant ID: 439...Reading Participant ID: 387...Reading Participant ID: 390...Reading Participant ID: 353...Reading Participant ID: 316...Reading Participant ID: 471...Reading Participant ID: 434...Reading Participant ID: 400...Reading Participant ID: 311...Reading Participant ID: 357...Reading Participant ID: 475...Reading Participant ID: 441...Reading Participant ID: 404...Reading Participant ID: 352...Reading Participant ID: 315...Reading Participant ID: 482...Reading Participant ID: 445...Reading Participant ID: 408...Reading Participant ID: 393...Reading Participant ID: 356...Reading Participant ID: 319...Reading Participant ID: 322...Reading Participant ID: 440...Reading Participant ID: 403...Reading Participant ID: 486...Reading Participant ID: 449...Reading Participant ID: 397...Reading Participant ID: 363...Reading Participant ID: 326...Reading Participant ID: 481...Reading Participant ID: 444...Reading Participant ID: 407...Reading Participant ID: 410...Reading Participant ID: 321...Unsuccessful frames in data to be removed: 3.835461318536759%

Unique videos: 189
Total processed frames: 2362555
Avg frames per video: 12500.291005291005
Memory used: 0.5192709900438786 GB

Preparing Dataloader
Preparing Data

Configuration saved to /ibm/gpfs/home/blea0003/Multi-Modal-Depression-Detection/depression_detection/models/au_only/logs/config.json

Epoch 1/15
------------------------------
Train Loss: 0.8890, Accuracy: 0.5133
Val Loss: 0.7183, Accuracy: 0.4208
learning rate: [0.001]

Epoch 2/15
------------------------------
Train Loss: 0.6986, Accuracy: 0.5226
Val Loss: 0.7806, Accuracy: 0.4083
learning rate: [0.0009990133642141358]

Epoch 3/15
------------------------------
Train Loss: 0.7057, Accuracy: 0.5273
Val Loss: 0.6848, Accuracy: 0.5708
learning rate: [0.000996057350657239]
Best loss model updated.
Best accuracy model updated.

Epoch 4/15
------------------------------
Train Loss: 0.7007, Accuracy: 0.5398
Val Loss: 0.6746, Accuracy: 0.6042
learning rate: [0.0009911436253643444]
Best loss model updated.
Best accuracy model updated.

Epoch 5/15
------------------------------
Train Loss: 0.6860, Accuracy: 0.5741
Val Loss: 0.6675, Accuracy: 0.6167
learning rate: [0.0009842915805643156]
Best loss model updated.
Best accuracy model updated.

Epoch 6/15
------------------------------
Train Loss: 0.6711, Accuracy: 0.5913
Val Loss: 0.6994, Accuracy: 0.5958
learning rate: [0.0009755282581475769]

Epoch 7/15
------------------------------
Train Loss: 0.5918, Accuracy: 0.6786
Val Loss: 0.7044, Accuracy: 0.6250
learning rate: [0.0009648882429441258]
Best accuracy model updated.

Epoch 8/15
------------------------------
Train Loss: 0.5404, Accuracy: 0.7301
Val Loss: 0.7872, Accuracy: 0.5792
learning rate: [0.0009524135262330099]

Epoch 9/15
------------------------------
Train Loss: 0.4614, Accuracy: 0.7691
Val Loss: 0.9137, Accuracy: 0.5917
learning rate: [0.0009381533400219318]

Epoch 10/15
------------------------------
Train Loss: 0.3908, Accuracy: 0.8222
Val Loss: 1.1213, Accuracy: 0.5417
learning rate: [0.0009221639627510075]

Epoch 11/15
------------------------------
Train Loss: 0.3044, Accuracy: 0.8674
Val Loss: 1.2664, Accuracy: 0.6042
learning rate: [0.0009045084971874736]

Epoch 12/15
------------------------------
Train Loss: 0.2648, Accuracy: 0.8939
Val Loss: 1.8350, Accuracy: 0.5875
learning rate: [0.0008852566213878945]

Epoch 13/15
------------------------------
Train Loss: 0.2967, Accuracy: 0.8830
Val Loss: 1.6858, Accuracy: 0.5333
learning rate: [0.0008644843137107056]

Epoch 14/15
------------------------------
Train Loss: 0.2017, Accuracy: 0.9267
Val Loss: 1.9164, Accuracy: 0.5917
learning rate: [0.0008422735529643443]

Epoch 15/15
------------------------------
Train Loss: 0.1422, Accuracy: 0.9485
Val Loss: 2.2108, Accuracy: 0.5708
learning rate: [0.0008187119948743448]
Training complete.

TESTING FINAL MODEL

Confusion Matrix
 [[176  64]
 [ 72  21]]

Classification Report
               precision    recall  f1-score   support

           0       0.71      0.73      0.72       240
           1       0.25      0.23      0.24        93

    accuracy                           0.59       333
   macro avg       0.48      0.48      0.48       333
weighted avg       0.58      0.59      0.59       333


TESTING BEST LOSS MODEL
Loaded model states from /ibm/gpfs/home/blea0003/Multi-Modal-Depression-Detection/depression_detection/models/au_only/checkpoints/best_loss_model.pth

Confusion Matrix
 [[182  58]
 [ 73  20]]

Classification Report
               precision    recall  f1-score   support

           0       0.71      0.76      0.74       240
           1       0.26      0.22      0.23        93

    accuracy                           0.61       333
   macro avg       0.49      0.49      0.48       333
weighted avg       0.59      0.61      0.60       333


TESTING BEST ACCURACY MODEL
Loaded model states from /ibm/gpfs/home/blea0003/Multi-Modal-Depression-Detection/depression_detection/models/au_only/checkpoints/best_acc_model.pth

Confusion Matrix
 [[111 129]
 [ 49  44]]

Classification Report
               precision    recall  f1-score   support

           0       0.69      0.46      0.56       240
           1       0.25      0.47      0.33        93

    accuracy                           0.47       333
   macro avg       0.47      0.47      0.44       333
weighted avg       0.57      0.47      0.49       333

