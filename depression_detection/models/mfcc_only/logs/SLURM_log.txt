Running Script: /ibm/gpfs/home/blea0003/Multi-Modal-Depression-Detection/depression_detection/models/mfcc_only
Reading DAIC-WOZ Data
Reading Participant ID: 367...Reading Participant ID: 485...Reading Participant ID: 448...Reading Participant ID: 451...Reading Participant ID: 414...Reading Participant ID: 362...Reading Participant ID: 325...Reading Participant ID: 320...Reading Participant ID: 489...Reading Participant ID: 492...Reading Participant ID: 455...Reading Participant ID: 418...Reading Participant ID: 366...Reading Participant ID: 329...Reading Participant ID: 450...Reading Participant ID: 413...Reading Participant ID: 361...Reading Participant ID: 459...Reading Participant ID: 336...Reading Participant ID: 491...Reading Participant ID: 454...Reading Participant ID: 417...Reading Participant ID: 331...Reading Participant ID: 377...Reading Participant ID: 458...Reading Participant ID: 424...Reading Participant ID: 372...Reading Participant ID: 335...Reading Participant ID: 490...Reading Participant ID: 330...Reading Participant ID: 465...Reading Participant ID: 428...Reading Participant ID: 376...Reading Participant ID: 339...Reading Participant ID: 423...Reading Participant ID: 371...Reading Participant ID: 334...Reading Participant ID: 300...Reading Participant ID: 469...Reading Participant ID: 309...Reading Participant ID: 464...Reading Participant ID: 427...Reading Participant ID: 375...Reading Participant ID: 341...Reading Participant ID: 304...Reading Participant ID: 422...Reading Participant ID: 468...Reading Participant ID: 382...Reading Participant ID: 345...Reading Participant ID: 308...Reading Participant ID: 463...Reading Participant ID: 340...Reading Participant ID: 303...Reading Participant ID: 438...Reading Participant ID: 386...Reading Participant ID: 349...Reading Participant ID: 470...Reading Participant ID: 433...Reading Participant ID: 381...Reading Participant ID: 344...Reading Participant ID: 307...Reading Participant ID: 310...Reading Participant ID: 479...Reading Participant ID: 474...Reading Participant ID: 437...Reading Participant ID: 385...Reading Participant ID: 348...Reading Participant ID: 351...Reading Participant ID: 314...Reading Participant ID: 432...Reading Participant ID: 478...Reading Participant ID: 389...Reading Participant ID: 392...Reading Participant ID: 355...Reading Participant ID: 318...Reading Participant ID: 473...Reading Participant ID: 436...Reading Participant ID: 402...Reading Participant ID: 350...Reading Participant ID: 313...Reading Participant ID: 396...Reading Participant ID: 359...Reading Participant ID: 477...Reading Participant ID: 480...Reading Participant ID: 443...Reading Participant ID: 406...Reading Participant ID: 391...Reading Participant ID: 354...Reading Participant ID: 317...Reading Participant ID: 401...Reading Participant ID: 484...Reading Participant ID: 447...Reading Participant ID: 395...Reading Participant ID: 358...Reading Participant ID: 324...Reading Participant ID: 442...Reading Participant ID: 405...Reading Participant ID: 488...Reading Participant ID: 399...Reading Participant ID: 365...Reading Participant ID: 328...Reading Participant ID: 483...Reading Participant ID: 446...Reading Participant ID: 409...Reading Participant ID: 412...Reading Participant ID: 360...Reading Participant ID: 323...Reading Participant ID: 369...Reading Participant ID: 487...Reading Participant ID: 453...Reading Participant ID: 416...Reading Participant ID: 364...Reading Participant ID: 327...Reading Participant ID: 411...Reading Participant ID: 457...Reading Participant ID: 368...Reading Participant ID: 452...Reading Participant ID: 415...Reading Participant ID: 338...Reading Participant ID: 456...Reading Participant ID: 419...Reading Participant ID: 370...Reading Participant ID: 333...Reading Participant ID: 379...Reading Participant ID: 426...Reading Participant ID: 374...Reading Participant ID: 337...Reading Participant ID: 421...Reading Participant ID: 332...Reading Participant ID: 467...Reading Participant ID: 378...Reading Participant ID: 462...Reading Participant ID: 425...Reading Participant ID: 373...Reading Participant ID: 302...Reading Participant ID: 420...Reading Participant ID: 466...Reading Participant ID: 429...Reading Participant ID: 380...Reading Participant ID: 343...Reading Participant ID: 306...Reading Participant ID: 461...Reading Participant ID: 301...Reading Participant ID: 384...Reading Participant ID: 347...Reading Participant ID: 431...Reading Participant ID: 305...Reading Participant ID: 388...Reading Participant ID: 472...Reading Participant ID: 435...Reading Participant ID: 383...Reading Participant ID: 346...Reading Participant ID: 312...Reading Participant ID: 430...Reading Participant ID: 476...Reading Participant ID: 439...Reading Participant ID: 387...Reading Participant ID: 390...Reading Participant ID: 353...Reading Participant ID: 316...Reading Participant ID: 471...Reading Participant ID: 434...Reading Participant ID: 400...Reading Participant ID: 311...Reading Participant ID: 357...Reading Participant ID: 475...Reading Participant ID: 441...Reading Participant ID: 404...Reading Participant ID: 352...Reading Participant ID: 315...Reading Participant ID: 482...Reading Participant ID: 445...Reading Participant ID: 408...Reading Participant ID: 393...Reading Participant ID: 356...Reading Participant ID: 319...Reading Participant ID: 322...Reading Participant ID: 440...Reading Participant ID: 403...Reading Participant ID: 486...Reading Participant ID: 449...Reading Participant ID: 397...Reading Participant ID: 363...Reading Participant ID: 326...Reading Participant ID: 481...Reading Participant ID: 444...Reading Participant ID: 407...Reading Participant ID: 410...Reading Participant ID: 321...
Unique videos: 189
Total processed frames: 2456784
Avg frames per video: 12998.857142857143
Memory used: 0.15558809414505959 GB

Preparing Dataloader
Preparing Data

Configuration saved to /ibm/gpfs/home/blea0003/Multi-Modal-Depression-Detection/depression_detection/models/mfcc_only/logs/config.json

Epoch 1/50
------------------------------
Train Loss: 0.7154, Accuracy: 0.5344
Val Loss: 0.7219, Accuracy: 0.4981
learning rate: [0.001]

Epoch 2/50
------------------------------
Train Loss: 0.6269, Accuracy: 0.6811
Val Loss: 0.8891, Accuracy: 0.4436
learning rate: [0.0009990133642141358]

Epoch 3/50
------------------------------
Train Loss: 0.5374, Accuracy: 0.7051
Val Loss: 1.0431, Accuracy: 0.4125
learning rate: [0.000996057350657239]
Best loss model updated.
Best accuracy model updated.

Epoch 4/50
------------------------------
Train Loss: 0.3832, Accuracy: 0.8159
Val Loss: 1.3620, Accuracy: 0.5175
learning rate: [0.0009911436253643444]
Best accuracy model updated.

Epoch 5/50
------------------------------
Train Loss: 0.2818, Accuracy: 0.8817
Val Loss: 1.6961, Accuracy: 0.4397
learning rate: [0.0009842915805643156]

Epoch 6/50
------------------------------
Train Loss: 0.2403, Accuracy: 0.9087
Val Loss: 1.7421, Accuracy: 0.4397
learning rate: [0.0009755282581475769]

Epoch 7/50
------------------------------
Train Loss: 0.1642, Accuracy: 0.9431
Val Loss: 2.6118, Accuracy: 0.4358
learning rate: [0.0009648882429441258]

Epoch 8/50
------------------------------
Train Loss: 0.0824, Accuracy: 0.9686
Val Loss: 2.7889, Accuracy: 0.3852
learning rate: [0.0009524135262330099]

Epoch 9/50
------------------------------
Train Loss: 0.0411, Accuracy: 0.9865
Val Loss: 4.1594, Accuracy: 0.4125
learning rate: [0.0009381533400219318]

Epoch 10/50
------------------------------
Train Loss: 0.0238, Accuracy: 0.9910
Val Loss: 4.4537, Accuracy: 0.4125
learning rate: [0.0009221639627510075]

Epoch 11/50
------------------------------
Train Loss: 0.0292, Accuracy: 0.9925
Val Loss: 4.7468, Accuracy: 0.4669
learning rate: [0.0009045084971874736]

Epoch 12/50
------------------------------
Train Loss: 0.0221, Accuracy: 0.9940
Val Loss: 4.0501, Accuracy: 0.3891
learning rate: [0.0008852566213878945]

Epoch 13/50
------------------------------
Train Loss: 0.0064, Accuracy: 1.0000
Val Loss: 5.1207, Accuracy: 0.4202
learning rate: [0.0008644843137107056]

Epoch 14/50
------------------------------
Train Loss: 0.0043, Accuracy: 1.0000
Val Loss: 5.8206, Accuracy: 0.4319
learning rate: [0.0008422735529643443]

Epoch 15/50
------------------------------
Train Loss: 0.0034, Accuracy: 0.9985
Val Loss: 6.7950, Accuracy: 0.4630
learning rate: [0.0008187119948743448]

Epoch 16/50
------------------------------
Train Loss: 0.0020, Accuracy: 1.0000
Val Loss: 7.0812, Accuracy: 0.4630
learning rate: [0.0007938926261462366]

Epoch 17/50
------------------------------
Train Loss: 0.0028, Accuracy: 1.0000
Val Loss: 7.0984, Accuracy: 0.4125
learning rate: [0.0007679133974894982]

Epoch 18/50
------------------------------
Train Loss: 0.0019, Accuracy: 1.0000
Val Loss: 6.9746, Accuracy: 0.4553
learning rate: [0.0007408768370508576]

Epoch 19/50
------------------------------
Train Loss: 0.0009, Accuracy: 1.0000
Val Loss: 7.3641, Accuracy: 0.4591
learning rate: [0.0007128896457825362]

Epoch 20/50
------------------------------
Train Loss: 0.0007, Accuracy: 1.0000
Val Loss: 7.6319, Accuracy: 0.4591
learning rate: [0.0006840622763423389]

Epoch 21/50
------------------------------
Train Loss: 0.0009, Accuracy: 1.0000
Val Loss: 7.3587, Accuracy: 0.4514
learning rate: [0.0006545084971874735]

Epoch 22/50
------------------------------
Train Loss: 0.0004, Accuracy: 1.0000
Val Loss: 7.3458, Accuracy: 0.4475
learning rate: [0.0006243449435824271]

Epoch 23/50
------------------------------
Train Loss: 0.0004, Accuracy: 1.0000
Val Loss: 7.4573, Accuracy: 0.4475
learning rate: [0.0005936906572928622]

Epoch 24/50
------------------------------
Train Loss: 0.0002, Accuracy: 1.0000
Val Loss: 7.5676, Accuracy: 0.4475
learning rate: [0.000562666616782152]

Epoch 25/50
------------------------------
Train Loss: 0.0005, Accuracy: 1.0000
Val Loss: 7.6530, Accuracy: 0.4475
learning rate: [0.0005313952597646566]

Epoch 26/50
------------------------------
Train Loss: 0.0002, Accuracy: 1.0000
Val Loss: 7.7871, Accuracy: 0.4514
learning rate: [0.0004999999999999998]

Epoch 27/50
------------------------------
Train Loss: 0.0002, Accuracy: 1.0000
Val Loss: 7.9012, Accuracy: 0.4514
learning rate: [0.00046860474023534314]

Epoch 28/50
------------------------------
Train Loss: 0.0002, Accuracy: 1.0000
Val Loss: 7.9725, Accuracy: 0.4475
learning rate: [0.00043733338321784774]

Epoch 29/50
------------------------------
Train Loss: 0.0001, Accuracy: 1.0000
Val Loss: 7.9867, Accuracy: 0.4436
learning rate: [0.00040630934270713756]

Epoch 30/50
------------------------------
Train Loss: 0.0004, Accuracy: 1.0000
Val Loss: 8.0159, Accuracy: 0.4436
learning rate: [0.00037565505641757246]

Epoch 31/50
------------------------------
Train Loss: 0.0004, Accuracy: 1.0000
Val Loss: 8.0887, Accuracy: 0.4436
learning rate: [0.00034549150281252633]

Epoch 32/50
------------------------------
Train Loss: 0.0002, Accuracy: 1.0000
Val Loss: 8.0484, Accuracy: 0.4319
learning rate: [0.00031593772365766105]

Epoch 33/50
------------------------------
Train Loss: 0.0002, Accuracy: 1.0000
Val Loss: 8.0676, Accuracy: 0.4319
learning rate: [0.00028711035421746355]

Epoch 34/50
------------------------------
Train Loss: 0.0007, Accuracy: 1.0000
Val Loss: 8.1875, Accuracy: 0.4319
learning rate: [0.0002591231629491422]

Epoch 35/50
------------------------------
Train Loss: 0.0002, Accuracy: 1.0000
Val Loss: 8.2515, Accuracy: 0.4358
learning rate: [0.00023208660251050145]

Epoch 36/50
------------------------------
Train Loss: 0.0005, Accuracy: 1.0000
Val Loss: 8.2621, Accuracy: 0.4358
learning rate: [0.00020610737385376337]

Epoch 37/50
------------------------------
Train Loss: 0.0001, Accuracy: 1.0000
Val Loss: 8.2694, Accuracy: 0.4397
learning rate: [0.00018128800512565502]

Epoch 38/50
------------------------------
Train Loss: 0.0002, Accuracy: 1.0000
Val Loss: 8.2920, Accuracy: 0.4358
learning rate: [0.00015772644703565555]

Epoch 39/50
------------------------------
Train Loss: 0.0002, Accuracy: 1.0000
Val Loss: 8.3002, Accuracy: 0.4358
learning rate: [0.00013551568628929425]

Epoch 40/50
------------------------------
Train Loss: 0.0001, Accuracy: 1.0000
Val Loss: 8.3010, Accuracy: 0.4319
learning rate: [0.00011474337861210535]

Epoch 41/50
------------------------------
Train Loss: 0.0001, Accuracy: 1.0000
Val Loss: 8.3207, Accuracy: 0.4319
learning rate: [9.549150281252626e-05]

Epoch 42/50
------------------------------
Train Loss: 0.0003, Accuracy: 1.0000
Val Loss: 8.3510, Accuracy: 0.4358
learning rate: [7.783603724899252e-05]

Epoch 43/50
------------------------------
Train Loss: 0.0001, Accuracy: 1.0000
Val Loss: 8.3732, Accuracy: 0.4319
learning rate: [6.184665997806817e-05]

Epoch 44/50
------------------------------
Train Loss: 0.0001, Accuracy: 1.0000
Val Loss: 8.3808, Accuracy: 0.4319
learning rate: [4.7586473766990294e-05]

Epoch 45/50
------------------------------
Train Loss: 0.0001, Accuracy: 1.0000
Val Loss: 8.3820, Accuracy: 0.4319
learning rate: [3.5111757055874305e-05]

Epoch 46/50
------------------------------
Train Loss: 0.0001, Accuracy: 1.0000
Val Loss: 8.3843, Accuracy: 0.4319
learning rate: [2.4471741852423218e-05]

Epoch 47/50
------------------------------
Train Loss: 0.0004, Accuracy: 1.0000
Val Loss: 8.3875, Accuracy: 0.4319
learning rate: [1.5708419435684507e-05]

Epoch 48/50
------------------------------
Train Loss: 0.0001, Accuracy: 1.0000
Val Loss: 8.3896, Accuracy: 0.4319
learning rate: [8.856374635655634e-06]

Epoch 49/50
------------------------------
Train Loss: 0.0002, Accuracy: 1.0000
Val Loss: 8.3904, Accuracy: 0.4319
learning rate: [3.942649342761115e-06]

Epoch 50/50
------------------------------
Train Loss: 0.0001, Accuracy: 1.0000
Val Loss: 8.3905, Accuracy: 0.4319
learning rate: [9.866357858642198e-07]
Training complete.

TESTING FINAL MODEL

Confusion Matrix
 [[184  73]
 [ 68  26]]

Classification Report
               precision    recall  f1-score   support

           0       0.73      0.72      0.72       257
           1       0.26      0.28      0.27        94

    accuracy                           0.60       351
   macro avg       0.50      0.50      0.50       351
weighted avg       0.60      0.60      0.60       351


TESTING BEST LOSS MODEL
Loaded model states from /ibm/gpfs/home/blea0003/Multi-Modal-Depression-Detection/depression_detection/models/mfcc_only/checkpoints/best_loss_model.pth

Confusion Matrix
 [[124 133]
 [ 45  49]]

Classification Report
               precision    recall  f1-score   support

           0       0.73      0.48      0.58       257
           1       0.27      0.52      0.36        94

    accuracy                           0.49       351
   macro avg       0.50      0.50      0.47       351
weighted avg       0.61      0.49      0.52       351


TESTING BEST ACCURACY MODEL
Loaded model states from /ibm/gpfs/home/blea0003/Multi-Modal-Depression-Detection/depression_detection/models/mfcc_only/checkpoints/best_acc_model.pth

Confusion Matrix
 [[210  47]
 [ 70  24]]

Classification Report
               precision    recall  f1-score   support

           0       0.75      0.82      0.78       257
           1       0.34      0.26      0.29        94

    accuracy                           0.67       351
   macro avg       0.54      0.54      0.54       351
weighted avg       0.64      0.67      0.65       351

